{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset.metric\n",
    "\n",
    "The library also provides a selection of metrics focusing in particular on:\n",
    "\n",
    "* providing a common API accross a range of NLP metrics,\n",
    "* providing metrics associated to some benchmark datasets provided by the libray such as GLUE or SQuAD,\n",
    "* providing access to recent and somewhat complex metrics such as BLEURT or BERTScore,\n",
    "* allowing simple use of metrics in distributed and large-scale settings.\n",
    "\n",
    "Metrics in the datasets library have a lot in common with how `datasets.Datasets` are loaded and provided using `datasets.load_dataset()`.\n",
    "\n",
    "Like datasets, metrics are added to the library as small scripts wrapping them in a common API.\n",
    "\n",
    "A `datasets.Metric` can be created from various source:\n",
    "\n",
    "* from a metric script provided on the [HuggingFace Hub](https://huggingface.co/metrics), or\n",
    "* from a metric script provide at a local path in the filesystem.\n",
    "\n",
    "In this section we detail these options to access metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy', 'bertscore', 'bleu', 'bleurt', 'coval', 'f1', 'gleu', 'glue', 'indic_glue', 'meteor', 'precision', 'recall', 'rouge', 'sacrebleu', 'seqeval', 'squad', 'squad_v2', 'xnli']\n"
     ]
    }
   ],
   "source": [
    "from datasets import list_metrics\n",
    "print(list_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a metric from the Hub we use the `datasets.load_metric()` command and give it the short name of the metric you would like to load as listed above.\n",
    "\n",
    "Let’s load the metric associated to the MRPC subset of the GLUE benchmark for Natural Language Understanding. You can explore this dataset and find more details about it on the online viewer here :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99c507582cc429cad067dd90e6cfaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1586.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# same as dataset, for certain meta-datasets like GLUE, \n",
    "# need to specify which dataset as an argument \n",
    "metric = load_metric('glue', 'mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
       "Args:\n",
       "    predictions: list of predictions to score.\n",
       "        Each translation should be tokenized into a list of tokens.\n",
       "    references: list of lists of references for each translation.\n",
       "        Each reference should be tokenized into a list of tokens.\n",
       "Returns: depending on the GLUE subset, one or several of:\n",
       "    \"accuracy\": Accuracy\n",
       "    \"f1\": F1 score\n",
       "    \"pearson\": Pearson Correlation\n",
       "    \"spearmanr\": Spearman Correlation\n",
       "    \"matthews_correlation\": Matthew Correlation\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating a model’s predictions with `datasets.Metric` involves just a couple of methods:\n",
    "\n",
    "1. `datasets.Metric.add()` and `datasets.Metric.add_batch()` are used to add pairs of predictions/reference (or just predictions if a metric doesn’t make use of references) to a temporary and memory efficient cache table,\n",
    "2. `datasets.Metric.compute()` then gather all the cached predictions and reference to compute the metric score.\n",
    "\n",
    "Note:\n",
    "\n",
    "* `datasets.Metric.add_batch()` require the use of named arguments to avoid the silent error of mixing predictions with references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of typical usage\n",
    "for batch in dataset:\n",
    "    inputs, references = batch\n",
    "    predictions = model(inputs)\n",
    "    metric.add_batch(predictions=predictions, references=references)\n",
    "score = metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions?\n",
    "\n",
    "[Using datasets.Metric with Trainer()](https://github.com/huggingface/datasets/issues/1592):\n",
    "    \n",
    "I was quite surprised in the Metric documentation I don't see how it can be used with `Trainer()`. That would be the most intuitive use case instead of having to iterate the batches and add predictions and references to the metric, then compute the metric manually. Ideally, any pre-built metrics can be added to `compute_metrics` argument of `Trainer()` and they will be calculated at an interval specified by `TrainingArguments.evaluation_strategy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
